{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set visualization style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Load Anonymized Data\n",
    "print(\"Step 1: Loading anonymized data...\")\n",
    "DATA_DIR = '../data'\n",
    "CALLS_PATH = os.path.join(DATA_DIR, 'call_created_anonymized.csv')\n",
    "CENSUS_PATH = os.path.join(DATA_DIR, 'census_check_anonymized.csv')\n",
    "\n",
    "try:\n",
    "    calls_df = pd.read_csv(CALLS_PATH, parse_dates=['original_timestamp'], low_memory=False)\n",
    "    census_df = pd.read_csv(CENSUS_PATH, parse_dates=['original_timestamp'], low_memory=False)\n",
    "    print(\"Anonymized data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure your anonymized CSV files are in the 'data' directory.\")\n",
    "\n",
    "# Display initial data shapes and head\n",
    "print(f\"Calls data shape: {calls_df.shape}\")\n",
    "print(f\"Census data shape: {census_df.shape}\")\n",
    "calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcaa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Process and Categorize Call Data\n",
    "print(\"\\nStep 2: Processing and categorizing call data...\")\n",
    "\n",
    "# --- Filter out 'Unknown' hospital data ---\n",
    "initial_rows = len(calls_df)\n",
    "print(f\"Rows before filtering 'Unknown' hospital: {initial_rows}\")\n",
    "calls_df = calls_df[calls_df['hospital_name'] != 'Unknown'].copy()\n",
    "print(f\"Removed {initial_rows - len(calls_df)} rows from 'Unknown' hospital.\")\n",
    "print(f\"Rows remaining: {len(calls_df)}\")\n",
    "\n",
    "\n",
    "# Select necessary columns for the model and drop rows with missing essential data\n",
    "calls_df = calls_df[['original_timestamp', 'organization_id', 'call_type']].copy()\n",
    "calls_df.dropna(subset=['original_timestamp', 'organization_id', 'call_type'], inplace=True)\n",
    "calls_df['call_type'] = calls_df['call_type'].astype(str)\n",
    "\n",
    "# categorize_call function (based on 128 unique types)\n",
    "def categorize_call(call_type):\n",
    "    \"\"\"Maps a specific call_type string to a broader category based on actual data.\"\"\"\n",
    "    call_type_lower = call_type.lower()\n",
    "    \n",
    "    # Category 1: Clinical\n",
    "    clinical_keywords = [\n",
    "        'pain', 'nausea', 'medication', 'glucose', 'sugar', 'insulin',\n",
    "        'diabetic', 'vomiting', 'beeping', 'dressing', 'ekg', 'tums',\n",
    "        'purewick', 'chg bath', 'charge nurse'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in clinical_keywords):\n",
    "        return 'Clinical'\n",
    "        \n",
    "    # Category 2: Mobility\n",
    "    mobility_keywords = [\n",
    "        'restroom', 'walk', 'bed', 'reposition', 'commode', 'bedpan',\n",
    "        'urinal', 'shower', 'bath', 'bathroom'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in mobility_keywords):\n",
    "        return 'Mobility'\n",
    "        \n",
    "    # Category 3: Basic Need / Food & Drink\n",
    "    basic_need_keywords = [\n",
    "        'thirsty', 'hungry', 'water', 'meal', 'food', 'snack', 'juice',\n",
    "        'coffee', 'tea', 'milk', 'soda', 'ice', 'jello', 'pudding',\n",
    "        'yogurt', 'cracker', 'cheese', 'ensure', 'fruit', 'sauce',\n",
    "        'peanut butter', 'cheerios', 'popsicle', 'tray'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in basic_need_keywords):\n",
    "        return 'Basic Need'\n",
    "\n",
    "    # Category 4: Housekeeping & Room Environment\n",
    "    housekeeping_keywords = [\n",
    "        'housekeeping', 'linen', 'towel', 'soap', 'floor', 'trash', 'paper',\n",
    "        'sanitizer', 'clean-up', 'pillowcase', 'sheets', 'blanket',\n",
    "        'gown', 'lights on / off', 'too hot', 'too cold', 'heat pack'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in housekeeping_keywords):\n",
    "        return 'Housekeeping'\n",
    "\n",
    "        \n",
    "    # Category 5: Other / Administrative\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to create a new 'call_category' column\n",
    "calls_df['call_category'] = calls_df['call_type'].apply(categorize_call)\n",
    "print(\"Call categories created.\")\n",
    "calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63336901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aggregate, Pivot, and Merge\n",
    "print(\"\\nStep 3: Aggregating calls and merging with census data...\")\n",
    "\n",
    "hourly_calls = calls_df.copy()\n",
    "hourly_calls['timestamp_hour'] = hourly_calls['original_timestamp'].dt.floor('h')\n",
    "\n",
    "# Group by organization, the new hour column, and the category\n",
    "hourly_calls_pivoted = hourly_calls.groupby(['organization_id', 'timestamp_hour', 'call_category']) \\\n",
    "                                   .size() \\\n",
    "                                   .unstack(fill_value=0) \\\n",
    "                                   .reset_index()\n",
    "\n",
    "# Prepare census data for merging\n",
    "census_cols = ['original_timestamp', 'organization_id', 'rooms_with_patients']\n",
    "census_df_clean = census_df[census_cols].copy()\n",
    "census_df_clean.dropna(inplace=True)\n",
    "census_df_clean.drop_duplicates(subset=['organization_id', 'original_timestamp'], keep='last', inplace=True)\n",
    "\n",
    "print(\"Preparing data for merge...\")\n",
    "\n",
    "# Ensure consistent data types\n",
    "hourly_calls_pivoted['organization_id'] = hourly_calls_pivoted['organization_id'].astype(int)\n",
    "census_df_clean['organization_id'] = census_df_clean['organization_id'].astype(int)\n",
    "\n",
    "# Round timestamps to nearest hour for exact matching\n",
    "hourly_calls_pivoted['timestamp_rounded'] = hourly_calls_pivoted['timestamp_hour'].dt.round('h')\n",
    "census_df_clean['timestamp_rounded'] = census_df_clean['original_timestamp'].dt.round('h')\n",
    "\n",
    "# Perform the merge\n",
    "final_df = pd.merge(\n",
    "    hourly_calls_pivoted,\n",
    "    census_df_clean[['organization_id', 'timestamp_rounded', 'rooms_with_patients']],\n",
    "    on=['organization_id', 'timestamp_rounded'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up the merged dataframe\n",
    "final_df.drop(columns=['timestamp_rounded'], inplace=True)\n",
    "final_df.dropna(subset=['rooms_with_patients'], inplace=True)\n",
    "\n",
    "# Filter out records where census is zero\n",
    "print(f\"Data merged successfully. Shape before cleaning: {final_df.shape}\")\n",
    "initial_rows = len(final_df)\n",
    "final_df = final_df[final_df['rooms_with_patients'] > 0].copy()\n",
    "\n",
    "print(f\"Final dataframe shape after cleaning: {final_df.shape}\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Outlier Removal and Visualizations (UPDATED)\n",
    "print(\"\\nStep 4: Removing outliers and generating visualizations...\")\n",
    "\n",
    "# Define the final list of categories\n",
    "call_categories = ['Clinical', 'Mobility', 'Basic Need', 'Housekeeping', 'Other']\n",
    "\n",
    "# Ensure all expected category columns exist in the DataFrame, filling with 0 if not\n",
    "for col in call_categories:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = 0\n",
    "\n",
    "# Create a 'total_calls' column for analysis\n",
    "final_df['total_calls'] = final_df[call_categories].sum(axis=1)\n",
    "\n",
    "# --- NEW Outlier Removal Step ---\n",
    "# Remove any single hour where the total number of calls on a unit exceeded 30\n",
    "print(f\"Shape before hourly outlier removal: {final_df.shape}\")\n",
    "initial_rows = len(final_df)\n",
    "\n",
    "final_df = final_df[final_df['total_calls'] <= 30].copy()\n",
    "\n",
    "rows_removed = initial_rows - len(final_df)\n",
    "print(f\"Removed {rows_removed} rows where total hourly calls were > 30.\")\n",
    "print(f\"Shape after hourly outlier removal: {final_df.shape}\")\n",
    "\n",
    "\n",
    "# Continue with visualizations using the cleaned final_df\n",
    "final_df['hour_of_day'] = final_df['timestamp_hour'].dt.hour\n",
    "final_df['day_of_week'] = final_df['timestamp_hour'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "\n",
    "# Visualization 1: Average Calls by Category per Day of the Week\n",
    "print(\"\\nGenerated Visualization 1: Average Calls by Category per Day of the Week.\")\n",
    "\n",
    "daily_category_totals = final_df.groupby('day_of_week')[call_categories].sum()\n",
    "unique_day_counts = final_df.groupby('day_of_week')['timestamp_hour'].apply(lambda x: x.dt.date.nunique())\n",
    "daily_category_avg = daily_category_totals.div(unique_day_counts, axis=0)\n",
    "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_category_avg.index = daily_category_avg.index.map(lambda x: day_names[x])\n",
    "daily_category_avg.plot(kind='bar', stacked=True, figsize=(14, 7),\n",
    "                        colormap='viridis')\n",
    "\n",
    "plt.title('Average Calls by Category per Day of the Week', fontsize=16)\n",
    "plt.xlabel('Day of the Week', fontsize=12)\n",
    "plt.ylabel('Average Number of Calls', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Call Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualization 2: Average Calls by Hour of Day\n",
    "print(\"Visualization 2: Average Calls by Hour.\")\n",
    "hourly_avg = final_df.groupby('hour_of_day')['total_calls'].mean()\n",
    "plt.figure(figsize=(12, 6))\n",
    "hourly_avg.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Call Volume by Hour of the Day', fontsize=16)\n",
    "plt.xlabel('Hour of Day (24-hour format)', fontsize=12)\n",
    "plt.ylabel('Average Number of Calls', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualization 3: Relationship between Patient Census and Call Volume\n",
    "print(\"Visualization 3: Call Volume vs. Patient Census.\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "final_df['rooms_with_patients'] = pd.to_numeric(final_df['rooms_with_patients'])\n",
    "sns.scatterplot(data=final_df, x='rooms_with_patients', y='total_calls', alpha=0.3)\n",
    "plt.title('Hourly Call Volume vs. Number of Patients', fontsize=16)\n",
    "plt.xlabel('Number of Rooms with Patients (Census)', fontsize=12)\n",
    "plt.ylabel('Total Calls in that Hour', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "print(\"\\nStep 5: Filtering data to the last 365 days for model training...\")\n",
    "\n",
    "# Find the most recent date in the dataset\n",
    "latest_date = final_df['timestamp_hour'].max()\n",
    "start_date = latest_date - pd.Timedelta(days=365)\n",
    "\n",
    "print(f\"Full data range: {final_df['timestamp_hour'].min()} to {latest_date}\")\n",
    "print(f\"Filtering to training data from {start_date} onwards.\")\n",
    "\n",
    "model_df = final_df[final_df['timestamp_hour'] >= start_date].copy()\n",
    "\n",
    "print(f\"Shape of full dataframe: {final_df.shape}\")\n",
    "print(f\"Shape of model dataframe (last year): {model_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f37940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "print(\"\\nStep 6: Engineering features for the model...\")\n",
    "\n",
    "# Create simple time-based features\n",
    "model_df['hour_of_day'] = model_df['timestamp_hour'].dt.hour\n",
    "model_df['day_of_week'] = model_df['timestamp_hour'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "# --- CRITICAL: One-Hot Encode the organization_id ---\n",
    "# This allows the model to learn unit-specific patterns\n",
    "org_dummies = pd.get_dummies(model_df['organization_id'], prefix='organization_id')\n",
    "model_df = pd.concat([model_df, org_dummies], axis=1)\n",
    "\n",
    "# Define our features (X) and targets (y)\n",
    "# Start with the base features\n",
    "features = [\n",
    "    'rooms_with_patients',\n",
    "    'hour_of_day',\n",
    "    'day_of_week'\n",
    "]\n",
    "# Add the new one-hot encoded organization columns to the feature list\n",
    "features.extend(org_dummies.columns)\n",
    "\n",
    "\n",
    "# Use the final list of categories as our targets\n",
    "targets = ['Clinical', 'Mobility', 'Basic Need', 'Housekeeping', 'Other']\n",
    "\n",
    "# Ensure the target columns exist and are numeric, fill NaNs just in case\n",
    "for col in targets:\n",
    "    if col not in model_df.columns:\n",
    "        model_df[col] = 0\n",
    "    model_df[col] = pd.to_numeric(model_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "X = model_df[features]\n",
    "y = model_df[targets]\n",
    "\n",
    "print(\"Features (X) and Targets (y) created.\")\n",
    "print(f\"Total number of features: {len(features)}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493673a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "print(\"\\nStep 7: Training and evaluating the model...\")\n",
    "\n",
    "# Sort by time before splitting to ensure chronological order \n",
    "model_df.sort_values('timestamp_hour', inplace=True)\n",
    "X = model_df[features]\n",
    "y = model_df[targets]\n",
    "\n",
    "# Split data chronologically (80% train, 20% test)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} rows\")\n",
    "print(f\"Testing set size: {len(X_test)} rows\")\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, min_samples_leaf=5)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model and print accuracy metrics\n",
    "print(\"\\n--- Model Evaluation (Accuracy Metrics) ---\")\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=targets)\n",
    "\n",
    "for category in targets:\n",
    "    mae = mean_absolute_error(y_test[category], y_pred_df[category])\n",
    "    r2 = r2_score(y_test[category], y_pred_df[category])\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.2f} calls\")\n",
    "    print(f\"  R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# Overall performance for total calls\n",
    "total_mae = mean_absolute_error(y_test.sum(axis=1), y_pred.sum(axis=1))\n",
    "print(f\"\\nOverall MAE for Total Calls: {total_mae:.2f} calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5565aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "print(\"\\nStep 8: Saving model and feature columns...\")\n",
    "\n",
    "# Define the paths to save the files in the new 'models' directory\n",
    "MODELS_DIR = '../models'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "model_path = os.path.join(MODELS_DIR, 'call_forecasting_model.pkl')\n",
    "columns_path = os.path.join(MODELS_DIR, 'model_feature_columns.pkl')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Save the list of feature columns\n",
    "joblib.dump(features, columns_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Feature columns saved to: {columns_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
