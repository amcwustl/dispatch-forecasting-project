{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Set visualization style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Load Anonymized Data\n",
    "print(\"Step 1: Loading anonymized data...\")\n",
    "DATA_DIR = '../data'\n",
    "CALLS_PATH = os.path.join(DATA_DIR, 'call_created_anonymized.csv')\n",
    "CENSUS_PATH = os.path.join(DATA_DIR, 'census_check_anonymized.csv')\n",
    "\n",
    "try:\n",
    "    calls_df = pd.read_csv(CALLS_PATH, parse_dates=['original_timestamp'], low_memory=False)\n",
    "    census_df = pd.read_csv(CENSUS_PATH, parse_dates=['original_timestamp'], low_memory=False)\n",
    "    print(\"Anonymized data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure your anonymized CSV files are in the 'data' directory.\")\n",
    "\n",
    "# Display initial data shapes and head\n",
    "print(f\"Calls data shape: {calls_df.shape}\")\n",
    "print(f\"Census data shape: {census_df.shape}\")\n",
    "calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcaa3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Process and Categorize Call Data\n",
    "print(\"\\nStep 2: Processing and categorizing call data...\")\n",
    "\n",
    "# Select necessary columns and drop rows with missing essential data\n",
    "calls_df = calls_df[['original_timestamp', 'organization_id', 'call_type']].copy()\n",
    "calls_df.dropna(subset=['original_timestamp', 'organization_id', 'call_type'], inplace=True)\n",
    "calls_df['call_type'] = calls_df['call_type'].astype(str)\n",
    "\n",
    "# categorize_call function (based on 128 unique types)\n",
    "def categorize_call(call_type):\n",
    "    \"\"\"Maps a specific call_type string to a broader category based on actual data.\"\"\"\n",
    "    call_type_lower = call_type.lower()\n",
    "    \n",
    "    # Category 1: Clinical\n",
    "    clinical_keywords = [\n",
    "        'pain', 'nausea', 'medication', 'glucose', 'sugar', 'insulin',\n",
    "        'diabetic', 'vomiting', 'beeping', 'dressing', 'ekg', 'tums',\n",
    "        'purewick', 'chg bath', 'charge nurse'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in clinical_keywords):\n",
    "        return 'Clinical'\n",
    "        \n",
    "    # Category 2: Mobility\n",
    "    mobility_keywords = [\n",
    "        'restroom', 'walk', 'bed', 'reposition', 'commode', 'bedpan',\n",
    "        'urinal', 'shower', 'bath', 'bathroom'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in mobility_keywords):\n",
    "        return 'Mobility'\n",
    "        \n",
    "    # Category 3: Basic Need / Food & Drink\n",
    "    basic_need_keywords = [\n",
    "        'thirsty', 'hungry', 'water', 'meal', 'food', 'snack', 'juice',\n",
    "        'coffee', 'tea', 'milk', 'soda', 'ice', 'jello', 'pudding',\n",
    "        'yogurt', 'cracker', 'cheese', 'ensure', 'fruit', 'sauce',\n",
    "        'peanut butter', 'cheerios', 'popsicle', 'tray'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in basic_need_keywords):\n",
    "        return 'Basic Need'\n",
    "\n",
    "    # Category 4: Housekeeping & Room Environment\n",
    "    housekeeping_keywords = [\n",
    "        'housekeeping', 'linen', 'towel', 'soap', 'floor', 'trash', 'paper',\n",
    "        'sanitizer', 'clean-up', 'pillowcase', 'sheets', 'blanket',\n",
    "        'gown', 'lights on / off', 'too hot', 'too cold', 'heat pack'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in housekeeping_keywords):\n",
    "        return 'Housekeeping'\n",
    "\n",
    "    # Category 5: Personal Care & Assistance\n",
    "    personal_care_keywords = [\n",
    "        'hygiene', 'oral care', 'brush teeth', 'shave', 'chapstick',\n",
    "        'extra hands', 'help feeding', 'help with baby', '1:1 feeds'\n",
    "    ]\n",
    "    if any(keyword in call_type_lower for keyword in personal_care_keywords):\n",
    "        return 'Personal Care'\n",
    "        \n",
    "    # Category 6: Other / Administrative\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the function to create a new 'call_category' column\n",
    "calls_df['call_category'] = calls_df['call_type'].apply(categorize_call)\n",
    "print(\"Call categories created.\")\n",
    "calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63336901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aggregate, Pivot, and Merge\n",
    "print(\"\\nStep 3: Aggregating calls and merging with census data...\")\n",
    "\n",
    "hourly_calls = calls_df.copy()\n",
    "hourly_calls['timestamp_hour'] = hourly_calls['original_timestamp'].dt.floor('h')\n",
    "\n",
    "# Group by organization, the new hour column, and the category\n",
    "hourly_calls_pivoted = hourly_calls.groupby(['organization_id', 'timestamp_hour', 'call_category']) \\\n",
    "                                   .size() \\\n",
    "                                   .unstack(fill_value=0) \\\n",
    "                                   .reset_index()\n",
    "\n",
    "# Prepare census data for merging\n",
    "census_cols = ['original_timestamp', 'organization_id', 'rooms_with_patients']\n",
    "census_df_clean = census_df[census_cols].copy()\n",
    "census_df_clean.dropna(inplace=True)\n",
    "census_df_clean.drop_duplicates(subset=['organization_id', 'original_timestamp'], keep='last', inplace=True)\n",
    "\n",
    "print(\"Preparing data for merge...\")\n",
    "\n",
    "# Ensure consistent data types\n",
    "hourly_calls_pivoted['organization_id'] = hourly_calls_pivoted['organization_id'].astype(int)\n",
    "census_df_clean['organization_id'] = census_df_clean['organization_id'].astype(int)\n",
    "\n",
    "# Round timestamps to nearest hour for exact matching\n",
    "hourly_calls_pivoted['timestamp_rounded'] = hourly_calls_pivoted['timestamp_hour'].dt.round('h')\n",
    "census_df_clean['timestamp_rounded'] = census_df_clean['original_timestamp'].dt.round('h')\n",
    "\n",
    "# Perform the merge\n",
    "final_df = pd.merge(\n",
    "    hourly_calls_pivoted,\n",
    "    census_df_clean[['organization_id', 'timestamp_rounded', 'rooms_with_patients']],\n",
    "    on=['organization_id', 'timestamp_rounded'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up the merged dataframe\n",
    "final_df.drop(columns=['timestamp_rounded'], inplace=True)\n",
    "final_df.dropna(subset=['rooms_with_patients'], inplace=True)\n",
    "\n",
    "print(f\"Data merged successfully! Final dataframe shape: {final_df.shape}\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Outlier Removal and Visualizations\n",
    "print(\"\\nStep 4: Removing outliers and generating visualizations...\")\n",
    "\n",
    "# Define the final list of categories\n",
    "call_categories = ['Clinical', 'Mobility', 'Basic Need', 'Housekeeping', 'Personal Care', 'Other']\n",
    "\n",
    "# Ensure all expected category columns exist in the DataFrame, filling with 0 if not\n",
    "for col in call_categories:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = 0\n",
    "\n",
    "# Create a 'total_calls' column for analysis\n",
    "final_df['total_calls'] = final_df[call_categories].sum(axis=1)\n",
    "\n",
    "# --- NEW: Outlier Removal Step ---\n",
    "print(\"Identifying and removing outlier days (total calls > 1000)...\")\n",
    "initial_rows = len(final_df)\n",
    "\n",
    "# Create a temporary 'date' column for daily grouping\n",
    "final_df['date'] = final_df['timestamp_hour'].dt.date\n",
    "\n",
    "# Calculate total calls per organization per day\n",
    "daily_counts = final_df.groupby(['organization_id', 'date'])['total_calls'].sum().reset_index()\n",
    "\n",
    "# Identify the outlier days\n",
    "outlier_days = daily_counts[daily_counts['total_calls'] > 1000]\n",
    "\n",
    "if not outlier_days.empty:\n",
    "    print(\"Found the following outlier days to remove:\")\n",
    "    print(outlier_days)\n",
    "    \n",
    "    # Create a multi-index of the outlier orgs and dates to filter them out\n",
    "    outlier_index = pd.MultiIndex.from_frame(outlier_days[['organization_id', 'date']])\n",
    "    \n",
    "    # Create a boolean mask to identify rows in the main dataframe that are part of an outlier day\n",
    "    is_outlier = pd.MultiIndex.from_frame(final_df[['organization_id', 'date']]).isin(outlier_index)\n",
    "    \n",
    "    # Filter the main dataframe by keeping rows that are NOT outliers\n",
    "    final_df = final_df[~is_outlier].copy()\n",
    "    \n",
    "    print(f\"Removed {initial_rows - len(final_df)} hourly rows corresponding to outlier days.\")\n",
    "else:\n",
    "    print(\"No days found with over 1000 calls. No outliers removed.\")\n",
    "\n",
    "# Drop the temporary 'date' column\n",
    "final_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "\n",
    "# Continue with visualizations using the cleaned final_df\n",
    "final_df['hour_of_day'] = final_df['timestamp_hour'].dt.hour\n",
    "\n",
    "# --- Visualization 1: Call Volume Over Time ---\n",
    "plt.figure(figsize=(15, 6))\n",
    "daily_calls = final_df.set_index('timestamp_hour')['total_calls'].resample('D').sum()\n",
    "daily_calls.plot()\n",
    "plt.title('Total Daily Call Volume Over Time (Outliers Removed)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Number of Calls', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Generated Visualization 1: Total Daily Call Volume.\")\n",
    "\n",
    "# --- Visualization 2: Average Calls by Hour of Day ---\n",
    "hourly_avg = final_df.groupby('hour_of_day')['total_calls'].mean()\n",
    "plt.figure(figsize=(12, 6))\n",
    "hourly_avg.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Call Volume by Hour of the Day (Outliers Removed)', fontsize=16)\n",
    "plt.xlabel('Hour of Day (24-hour format)', fontsize=12)\n",
    "plt.ylabel('Average Number of Calls', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Generated Visualization 2: Average Calls by Hour.\")\n",
    "\n",
    "# --- Visualization 3: Relationship between Patient Census and Call Volume ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "final_df['rooms_with_patients'] = pd.to_numeric(final_df['rooms_with_patients'])\n",
    "sns.scatterplot(data=final_df, x='rooms_with_patients', y='total_calls', alpha=0.3)\n",
    "plt.title('Hourly Call Volume vs. Number of Patients (Outliers Removed)', fontsize=16)\n",
    "plt.xlabel('Number of Rooms with Patients (Census)', fontsize=12)\n",
    "plt.ylabel('Total Calls in that Hour', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Generated Visualization 3: Call Volume vs. Patient Census.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f37940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering\n",
    "print(\"\\nStep 5: Engineering features for the model...\")\n",
    "\n",
    "# Add any additional time-based features\n",
    "final_df['day_of_week'] = final_df['timestamp_hour'].dt.dayofweek # Monday=0, Sunday=6\n",
    "final_df['month'] = final_df['timestamp_hour'].dt.month\n",
    "\n",
    "# Define our features (X) and targets (y)\n",
    "features = [\n",
    "    'rooms_with_patients',\n",
    "    'hour_of_day',\n",
    "    'day_of_week',\n",
    "    'month'\n",
    "]\n",
    "\n",
    "# Use the final list of categories as our targets\n",
    "targets = ['Clinical', 'Mobility', 'Basic Need', 'Housekeeping', 'Personal Care', 'Other']\n",
    "\n",
    "# Ensure the target columns exist and are numeric, fill NaNs just in case\n",
    "for col in targets:\n",
    "    if col not in final_df.columns:\n",
    "        final_df[col] = 0\n",
    "    final_df[col] = pd.to_numeric(final_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "X = final_df[features]\n",
    "y = final_df[targets]\n",
    "\n",
    "print(\"Features (X) and Targets (y) created.\")\n",
    "print(\"Features used for model:\", features)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493673a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Training and Evaluation\n",
    "print(\"\\nStep 6: Training and evaluating the model...\")\n",
    "\n",
    "# Sort by time before splitting to ensure chronological order\n",
    "final_df.sort_values('timestamp_hour', inplace=True)\n",
    "X = final_df[features]\n",
    "y = final_df[targets]\n",
    "\n",
    "# Split data chronologically (80% train, 20% test)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} rows\")\n",
    "print(f\"Testing set size: {len(X_test)} rows\")\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, min_samples_leaf=5)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model and print accuracy metrics\n",
    "print(\"\\n--- Model Evaluation (Accuracy Metrics) ---\")\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=targets)\n",
    "\n",
    "for category in targets:\n",
    "    mae = mean_absolute_error(y_test[category], y_pred_df[category])\n",
    "    r2 = r2_score(y_test[category], y_pred_df[category])\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.2f} calls\")\n",
    "    print(f\"  R-squared (RÂ²): {r2:.2f}\")\n",
    "\n",
    "# Overall performance for total calls\n",
    "total_mae = mean_absolute_error(y_test.sum(axis=1), y_pred.sum(axis=1))\n",
    "print(f\"\\nOverall MAE for Total Calls: {total_mae:.2f} calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5565aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save Model Artifacts\n",
    "print(\"\\nStep 7: Saving model and feature columns...\")\n",
    "\n",
    "# Define the paths to save the files in the 'app' directory\n",
    "APP_DIR = '../app'\n",
    "if not os.path.exists(APP_DIR):\n",
    "    os.makedirs(APP_DIR)\n",
    "\n",
    "model_path = os.path.join(APP_DIR, 'call_forecasting_model.pkl')\n",
    "columns_path = os.path.join(APP_DIR, 'model_feature_columns.pkl')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Save the list of feature columns\n",
    "joblib.dump(features, columns_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Feature columns saved to: {columns_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
